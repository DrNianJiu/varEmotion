{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from prior_networks import UViT_Clip\n",
    "from prior_pipe import PriorPipe\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load prior diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_prior = UViT_Clip(embed_dim=512, num_heads=8, mlp_ratio=4)\n",
    "# number of parameters\n",
    "sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad)\n",
    "pipe = PriorPipe(diffusion_prior, device=device)\n",
    "# load pretrained model\n",
    "model_name = 'uvit_vice_pred_imagenet' \n",
    "path = f'ckpts/{model_name}'\n",
    "pipe.diffusion_prior.load_state_dict(torch.load(f'{path}.pt'))\n",
    "pipe.ema.load_state_dict(torch.load(f'{path}_ema.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract image features\n",
    "from diffusers.utils import load_image\n",
    "from IPython.display import Image, display\n",
    "from customized_pipe import Generator4Embeds, encode_image\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor,CLIPTextModelWithProjection,CLIPProcessor,AutoTokenizer\n",
    "import torch\n",
    "\n",
    "feature_extractor = CLIPImageProcessor()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    # \"h94/IP-Adapter-FaceID\", \n",
    "    \"h94/IP-Adapter\", \n",
    "    subfolder=\"models/image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "    'laion/CLIP-ViT-H-14-laion2B-s32B-b79K'\n",
    ").to(\"cuda\")\n",
    "pipe_image = Generator4Embeds(path='stabilityai/sdxl-turbo', num_inference_steps=4)\n",
    "# pipe_image = Generator4Embeds(path='stabilityai/stable-diffusion-xl-base-1.0', num_inference_steps=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_image.pipe.feature_extractor = feature_extractor\n",
    "pipe_image.pipe.image_encoder = image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.schedulers import DDPMScheduler\n",
    "scheduler = DDPMScheduler(\n",
    "    thresholding=False,\n",
    "    clip_sample=False,\n",
    ")\n",
    "pipe.scheduler = scheduler\n",
    "print((\n",
    "    pipe.scheduler.config.thresholding, \n",
    "    pipe.scheduler.config.sample_max_value, \n",
    "    pipe.scheduler.config.clip_sample,\n",
    "    pipe.scheduler.config.clip_sample_range\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image, make_image_grid\n",
    "# extract a concept embedding\n",
    "\n",
    "image_prompt = load_image('Your prior image')\n",
    "\n",
    "image_prompt = image_prompt.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "\n",
    "from IPython.display import display\n",
    "image_embeds = encode_image([image_prompt], image_encoder, feature_extractor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_head(nn.Module):\n",
    "    def __init__(self, input_dim=1024, output_dim=9):\n",
    "        super(MLP_head, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(input_dim, output_dim)) # (input_dim, output_dim)\n",
    "    \n",
    "    def init_weights(self, weight):\n",
    "        # weight: (output_dim, input_dim)\n",
    "        self.W.data = weight.T\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        # linear transformation\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        x = torch.matmul(x, self.W)\n",
    "        return x\n",
    "\n",
    "def calculate_img_txt_sim(image_features,text_features):\n",
    "    # print('This is image_feature.shape in calculate_img_txt_sim')\n",
    "    # print(image_features.shape)\n",
    "    image_features = image_features/image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features/text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (image_features @ text_features.T) # 计算相似度分数\n",
    "    return similarity\n",
    "\n",
    "def custom_cross_entropy(logits, p_target):\n",
    "    return - logits * p_target\n",
    "\n",
    "def get_loss_dim(net_mlp, p_target):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        net_mlp: MLP 1024->6\n",
    "        p_target (torch.Tensor): Tensor of shape (B, ), 目标的概率分布\n",
    "    Returns:\n",
    "        callable: A loss function that calculates the mean squared error loss over the specified concept dimensions for each sample in the batch.\n",
    "    \"\"\"\n",
    "    def loss_dim(x):\n",
    "        \n",
    "        c_pre = net_mlp(x)#16*7 16代表16张图，7代表与每个标准表征的相似度, (N, n_category)\n",
    "        \n",
    "        loss = custom_cross_entropy(c_pre, p_target) \n",
    "        \n",
    "        return loss.mean()\n",
    "    return loss_dim\n",
    "\n",
    "def get_loss_smooth(n_diag=2, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Returns a loss function that calculates the mean squared error (MSE) loss over a target similarity matrix.\n",
    "\n",
    "    The target similarity matrix is structured such that the first and second off-diagonals are set to 1, while the\n",
    "    main diagonal and other off-diagonal elements are ignored. This is useful for tasks where the similarity between\n",
    "    neighboring samples in the batch is to be maximized.\n",
    "\n",
    "    Returns:\n",
    "        callable: A loss function that computes the MSE loss between the computed similarity matrix and the target\n",
    "        similarity matrix for a given batch of inputs.\n",
    "    \"\"\"\n",
    "    def loss_smooth(x):\n",
    "        N = x.size(0)\n",
    "        assert n_diag < N, f\"n_diag must be less than the batch size N={N}\"\n",
    "\n",
    "        # Compute similarity matrix\n",
    "        sim = F.cosine_similarity(x.unsqueeze(1), x.unsqueeze(0), dim=-1)\n",
    "\n",
    "        # Initialize target similarity matrix with NaNs to ignore in loss calculation\n",
    "        target_sim = torch.full((N, N), 0.0, device=x.device)\n",
    "\n",
    "        # Set target similarities on the upper triangular of the first n off-diagonals to 1\n",
    "        for i in range(1, n_diag + 1):\n",
    "            target_sim += torch.diag(torch.full((N - i,), 1., device=x.device), i)\n",
    "            # target_sim += torch.diag(torch.full((N - i,), 1.0, device=x.device), -i) \n",
    "\n",
    "        # Mask out NaN values in target_sim for loss calculation\n",
    "        valid_mask = target_sim != 0\n",
    "        valid_mask[sim>=threshold] = False\n",
    "        # in case of no valid mask\n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=x.device, requires_grad=True)\n",
    "        loss = F.mse_loss(sim[valid_mask], target_sim[valid_mask])\n",
    "        # loss = F.mse_loss(sim.view(-1), target_sim.view(-1))\n",
    "        return loss.mean()\n",
    "\n",
    "    return loss_smooth\n",
    "\n",
    "def get_loss_similarity(h_target, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Returns a loss function that calculates the mean squared error (MSE) loss for the cosine similarity between each input\n",
    "    and a target embedding, but ignores those where the similarity exceeds a specified threshold.\n",
    "\n",
    "    Args:\n",
    "        h_target (torch.Tensor): Target embedding tensor of shape (1, D), where D is the dimensionality of the embeddings.\n",
    "        threshold (float): Similarity threshold above which no loss is computed.\n",
    "\n",
    "    Returns:\n",
    "        callable: A loss function that computes the MSE loss for inputs similar to the target below a certain threshold.\n",
    "    \"\"\"\n",
    "    def loss_similarity(x):\n",
    "        # Calculate the cosine similarity between the batch x and the target h_target\n",
    "        sim = F.cosine_similarity(x, h_target.repeat(x.size(0), 1), dim=1)\n",
    "        \n",
    "        # Apply threshold: Only consider embeddings with a similarity below the threshold\n",
    "        mask = sim < threshold\n",
    "        \n",
    "        # If all embeddings exceed the threshold, return zero loss\n",
    "        if mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=x.device, requires_grad=True)\n",
    "        \n",
    "        # Compute MSE loss for the selected embeddings below the threshold\n",
    "        target_sim_values = torch.ones_like(sim[mask])\n",
    "        loss = F.mse_loss(sim[mask], target_sim_values)\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "    return loss_similarity\n",
    "\n",
    "    \n",
    "def fns_collector(fns, scales):\n",
    "    \"\"\"\n",
    "    Combines multiple functions with corresponding scales into a single loss function.\n",
    "    \n",
    "    Args:\n",
    "    fns (list[callable]): List of function objects, each accepting the same type of input.\n",
    "    scales (list[float]): List of scaling factors for each function in `fns`.\n",
    "    \n",
    "    Returns:\n",
    "    callable: A combined function that computes the scaled sum of individual functions.\n",
    "    \"\"\"\n",
    "    def loss_func(x):\n",
    "        # Compute the weighted sum of functions\n",
    "        loss = sum(scale * fn(x) for scale, fn in zip(scales, fns))\n",
    "        return loss.mean()\n",
    "\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['surprise','fear','disgust','happiness','sadness','anger']\n",
    "texts = []\n",
    "for word in words:\n",
    "    texts.append('expression of very'+word)\n",
    "\n",
    "text_inputs = tokenizer(text=texts,padding=True,return_tensors='pt').to('cuda')\n",
    "#print(text_inputs)\n",
    "text_output = text_encoder(**text_inputs)\n",
    "text_features = text_output.text_embeds\n",
    "text_features.shape\n",
    "\n",
    "n = 18\n",
    "\n",
    "weights = torch.linspace(0,1,steps=n).to('cuda')\n",
    "\n",
    "feature_dimension = 3\n",
    "\n",
    "weights = torch.ones(n).float().to(device)\n",
    "\n",
    "feature_dimension = torch.arange(6).repeat(3)\n",
    "p_target = torch.zeros(n, 6, device=device)\n",
    "p_target[torch.arange(18), feature_dimension] = 1\n",
    "print(p_target)\n",
    "\n",
    "p_target[torch.arange(6), torch.arange(6)] = 0.33\n",
    "p_target[torch.arange(6,12), torch.arange(6)] = 0.66\n",
    "p_target[torch.arange(12,18), torch.arange(6)] = 1\n",
    "print(p_target)\n",
    "\n",
    "feature_dimension2 = [1,2]\n",
    "weights2 = torch.stack([weights,torch.flip(weights,dims=[0])],dim=0)\n",
    "print('This is weights2.shape')\n",
    "print(weights2.shape)\n",
    "print(f'feature dimension:{feature_dimension}')\n",
    "print(f'feature dimension2:{feature_dimension2}')\n",
    "\n",
    "generators = [torch.Generator(device=device).manual_seed(1) for _ in range(n)]\n",
    "seed_value = 42\n",
    "for generator in generators:\n",
    "    generator.manual_seed(seed_value)\n",
    "\n",
    "net_mlp = MLP_head(output_dim=6)\n",
    "net_mlp.load_state_dict(torch.load('MLP_head/MLP_head_beforeFN.pt'))\n",
    "net_mlp = net_mlp.to(device)\n",
    "\n",
    "def controversial_generate(n, generators, seed_value, p_target, net_mlp):\n",
    "    loss_dim = get_loss_dim(net_mlp,p_target)\n",
    "    loss_smooth = get_loss_smooth(n_diag=3, threshold=1)\n",
    "    loss_similarity = get_loss_similarity(h_target=image_embeds[0:1], threshold=1)\n",
    "\n",
    "    loss_func = fns_collector(\n",
    "    fns=[\n",
    "        loss_dim,\n",
    "        loss_smooth,\n",
    "        loss_similarity\n",
    "    ],\n",
    "    scales=[1, 0, 10]\n",
    "        # scales=[10, 3]\n",
    "        # scales = [1]\n",
    ")\n",
    "\n",
    "#下方生成了embeddings,h就是16*1024的embeddings,代表每个图片的embeddings\n",
    "#用下方生成的embeddings->h通过sdxl生成图片即可\n",
    "# Generate all embeddings in one pass\n",
    "    h = pipe.generate_guidance(\n",
    "    loss_fn=loss_func,\n",
    "    num_inference_steps=50,\n",
    "    num_resampling_steps=5,\n",
    "    guidance_scale=1,\n",
    "    generator=None,\n",
    "    use_ema=False,\n",
    "    latent=None,\n",
    "    strength=1,\n",
    "    N=n,\n",
    "    shape=(1024,)\n",
    ")\n",
    "\n",
    "    print(h.shape)\n",
    "    print(h)\n",
    "    print(F.softmax(net_mlp(h)))\n",
    "    print(net_mlp(h).shape)\n",
    "\n",
    "# Generate all images in parallel\n",
    "    pipe_image.pipe.set_ip_adapter_scale(1)\n",
    "    text_prompt = 'normal human face,well-formed, beautiful, correct proportions, high resolution, good anatomy, best quality, high quality.'\n",
    "    negative_prompt = \"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality,inhuman, white cracks and lines on the face, deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality,lines on the face,white cracks\"\n",
    "\n",
    "    from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n",
    "    pipe_image.pipe = AutoPipelineForImage2Image.from_pipe(pipe_image.pipe).to(\"cuda\")\n",
    "    images = []\n",
    "\n",
    "    for i in range(len(h)):\n",
    "        generators[i].manual_seed(seed_value)\n",
    "        image = pipe_image.generate(\n",
    "        h[i:i+1].to(dtype=torch.float16),\n",
    "        image=image_prompt.resize((512, 512)),\n",
    "        strength=0.5, \n",
    "        text_prompt=text_prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        generator=None,\n",
    "        # guidance_scale=1.25,\n",
    "    )\n",
    "        images.append(image[0])\n",
    "    return h,images\n",
    "\n",
    "num = 5 #对抗分5级\n",
    "dim = [0,1]\n",
    "p_target = torch.zeros(num,6,device=device)\n",
    "for i in range(num):\n",
    "    p_target[i,dim[0]] = i / 4\n",
    "    p_target[i,dim[1]] = 1-i / 4\n",
    "p_target\n",
    "\n",
    "h,images_list = controversial_generate(num,generators,42,p_target,net_mlp)\n",
    "display(make_image_grid([*images_list], rows=1, cols=5, resize=512))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
